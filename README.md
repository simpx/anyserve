# AnyServe

High-performance model serving framework with C++ Dispatcher and Python Worker architecture, supporting the KServe v2 inference protocol.

## Features

- **ðŸš€ High Performance**: C++ gRPC ingress for request routing and traffic handling
- **ðŸ Python Simplicity**: Write model handlers in pure Python with decorators
- **ðŸ”Œ KServe Compatible**: Full support for KServe v2 inference protocol
- **ðŸ“Š Multi-Model**: Serve multiple models with version support in a single deployment
- **ðŸ”„ Dynamic Registration**: Workers register models at runtime via gRPC
- **âš¡ Unix Socket IPC**: High-speed inter-process communication between Ingress and Workers

## Architecture

```
External Clients (gRPC)
        â†“
   C++ Dispatcher (Port 8000)
   â”œâ”€ Model Registry
   â”œâ”€ Request Router
   â””â”€ Worker Client
        â†“ (Unix Socket)
   Python Workers
   â””â”€ Model Handlers (@model decorator)
```

AnyServe uses a **C++ Dispatcher + Python Worker** architecture:
- **C++ Dispatcher**: Handles all external gRPC traffic, routes requests to appropriate workers
- **Python Workers**: Independent processes running your model inference code
- **Communication**: gRPC for management, Unix Domain Sockets for high-speed inference

For detailed architecture, see:
- [System Architecture](docs/architecture.md) - Overall design and concepts
- [Runtime Architecture](docs/runtime.md) - Implementation details

## Quick Start

### Prerequisites

- Python 3.11+
- C++ compiler with C++17 support
- CMake 3.20+
- Conan 2.0+ (for C++ dependencies)

### Installation

```bash
# Install dependencies and build
just setup
just build

# Install Python package
pip install -e python/
```

### Example: Echo Model

Create `my_app.py`:

```python
from anyserve import AnyServe, ModelInferRequest, ModelInferResponse

app = AnyServe()

@app.model("echo")
def echo_handler(request: ModelInferRequest) -> ModelInferResponse:
    """Echo back all inputs as outputs"""
    response = ModelInferResponse(
        model_name=request.model_name,
        id=request.id
    )

    for inp in request.inputs:
        out = response.add_output(
            name=f"output_{inp.name}",
            datatype=inp.datatype,
            shape=inp.shape
        )
        out.contents = inp.contents

    return response
```

### Run the Server

```bash
# Start server with 1 worker
python -m anyserve.cli my_app:app --port 8000 --workers 1
```

### Test the Model

```bash
# Using the test client
python examples/basic/run_example.py
```

Or use the Python client:

```python
import grpc
from anyserve._proto import grpc_predict_v2_pb2
from anyserve._proto import grpc_predict_v2_pb2_grpc

channel = grpc.insecure_channel('localhost:8000')
stub = grpc_predict_v2_pb2_grpc.GRPCInferenceServiceStub(channel)

# Check server status
server_live = stub.ServerLive(grpc_predict_v2_pb2.ServerLiveRequest())
print(f"Server live: {server_live.live}")

# Check model status
model_ready = stub.ModelReady(
    grpc_predict_v2_pb2.ModelReadyRequest(name="echo")
)
print(f"Model ready: {model_ready.ready}")

# Make inference request
request = grpc_predict_v2_pb2.ModelInferRequest()
request.model_name = "echo"
request.id = "test-1"

input_tensor = request.inputs.add()
input_tensor.name = "input"
input_tensor.datatype = "INT32"
input_tensor.shape.extend([3])
input_tensor.contents.int_contents.extend([1, 2, 3])

response = stub.ModelInfer(request)
print(f"Response: {response}")
```

## Development

### Project Structure

```
anyserve/
â”œâ”€â”€ cpp/                    # C++ Dispatcher implementation
â”‚   â”œâ”€â”€ server/            # Core server components
â”‚   â”‚   â”œâ”€â”€ anyserve_ingress.{cpp,hpp}   # Main ingress server
â”‚   â”‚   â”œâ”€â”€ model_registry.{cpp,hpp}      # Model registry
â”‚   â”‚   â””â”€â”€ worker_client.{cpp,hpp}       # Unix socket client
â”‚   â””â”€â”€ build/             # Build artifacts (gitignored)
â”œâ”€â”€ python/anyserve/       # Python library
â”‚   â”œâ”€â”€ cli.py            # CLI entry point
â”‚   â”œâ”€â”€ kserve.py         # KServe v2 protocol
â”‚   â””â”€â”€ worker/           # Worker implementation
â”œâ”€â”€ proto/                 # Protocol definitions
â”‚   â”œâ”€â”€ grpc_predict_v2.proto      # KServe v2 protocol
â”‚   â””â”€â”€ worker_management.proto     # Worker registration
â”œâ”€â”€ examples/             # Example applications
â”‚   â””â”€â”€ basic/           # Basic examples
â”œâ”€â”€ docs/                # Documentation
â””â”€â”€ justfile            # Build and development commands
```

### Build Commands

```bash
# Setup environment (install Conan dependencies)
just setup

# Build C++ components
just build

# Clean build artifacts
just clean

# Run tests (coming soon)
# just test
```

### Documentation

- [System Architecture](docs/architecture.md) - High-level system design
- [Runtime Architecture](docs/runtime.md) - Implementation details and component interactions
- [MVP Specification](docs/mvp.md) - Project scope and goals
- [Agent Guide](agents.md) - AI assistant collaboration guide

## Examples

See the [examples/](examples/) directory for complete examples:

- `basic/` - Basic model serving with echo, add, and classifier models
- `multi_stage/` - Multi-stage pipelines (placeholder for future)
- `streaming/` - Streaming responses (placeholder for future)

## Contributing

This project uses AI-assisted development. See [agents.md](agents.md) for collaboration guidelines.

## License

[Add your license here]

## Status

âœ… **Core Features Complete**
- C++ Dispatcher server with gRPC and Unix Socket support
- Python Worker with KServe v2 protocol
- Dynamic model registration
- Multi-model serving with versioning

ðŸš§ **In Progress**
- Performance optimization
- Monitoring and metrics
- Advanced load balancing

ðŸ“‹ **Planned**
- Streaming inference support
- Model auto-scaling
- Distributed deployment
