"""
Worker C - Format Stage

This is the final stage in the pipeline. It:
1. Receives analysis data (BYTES JSON) and statistics (FP32/INT32)
2. Formats the data into a human-readable report
3. Returns the final report

Port: 50053
Capability: type="format"

Usage:
    anyserve examples.pipeline.worker_c:app --port 50053 --api-server http://localhost:8080
"""

import json

import anyserve
from anyserve import ModelInferRequest, ModelInferResponse, Context

app = anyserve.AnyServe()


def format_report(analysis: dict, text_snippet: str, avg_length: float, total: int, unique: int) -> str:
    """Format analysis data into a human-readable report."""
    lines = [
        "=" * 50,
        "         TEXT ANALYSIS REPORT",
        "=" * 50,
        "",
        "INPUT TEXT PREVIEW:",
        f"  \"{text_snippet[:100]}{'...' if len(text_snippet) > 100 else ''}\"",
        "",
        "STATISTICS:",
        f"  Total tokens:     {total}",
        f"  Unique tokens:    {unique}",
        f"  Avg token length: {avg_length:.2f} characters",
        f"  Vocabulary ratio: {(unique/total*100 if total > 0 else 0):.1f}%",
        "",
    ]

    # Add top tokens section
    top_tokens = analysis.get("top_tokens", [])
    if top_tokens:
        lines.append("TOP 5 MOST FREQUENT TOKENS:")
        for i, item in enumerate(top_tokens, 1):
            token = item.get("token", "?")
            count = item.get("count", 0)
            bar = "#" * min(count, 20)
            lines.append(f"  {i}. '{token}' ({count}x) {bar}")
        lines.append("")

    lines.extend([
        "=" * 50,
        "Report generated by AnyServe Pipeline",
        "=" * 50,
    ])

    return "\n".join(lines)


@app.capability(type="format")
def format_handler(request: ModelInferRequest, context: Context) -> ModelInferResponse:
    """
    Format capability - formats analysis into a report.

    Input:
        - analysis_json: BYTES (JSON string with analysis data)
        - text_snippet: BYTES (original text snippet)
        - avg_token_length: FP32 (average token length)
        - total_tokens: INT32 (total token count)
        - unique_tokens: INT32 (unique token count)

    Output:
        - report: BYTES (formatted report string)
    """
    print(f"[format] Processing request {request.id}")

    # 1. Get inputs
    analysis_input = request.get_input("analysis_json")
    text_input = request.get_input("text_snippet")
    avg_length_input = request.get_input("avg_token_length")
    total_input = request.get_input("total_tokens")
    unique_input = request.get_input("unique_tokens")

    # Parse analysis JSON
    analysis = {}
    if analysis_input and analysis_input.bytes_contents:
        analysis_str = analysis_input.bytes_contents[0].decode('utf-8')
        analysis = json.loads(analysis_str)

    # Get text snippet
    text_snippet = ""
    if text_input and text_input.bytes_contents:
        text_snippet = text_input.bytes_contents[0].decode('utf-8')

    # Get numeric values
    avg_length = 0.0
    if avg_length_input and avg_length_input.fp32_contents:
        avg_length = avg_length_input.fp32_contents[0]

    total = 0
    if total_input and total_input.int_contents:
        total = total_input.int_contents[0]

    unique = 0
    if unique_input and unique_input.int_contents:
        unique = unique_input.int_contents[0]

    print(f"[format] Formatting report: total={total}, unique={unique}, avg_len={avg_length}")

    # 2. Format the report
    report = format_report(analysis, text_snippet, avg_length, total, unique)
    print(f"[format] Generated report ({len(report)} chars)")

    # 3. Build response
    response = ModelInferResponse(
        model_name=request.model_name,
        model_version=request.model_version,
        id=request.id,
    )

    response.add_output(
        name="report",
        datatype="BYTES",
        shape=[1],
        bytes_contents=[report.encode('utf-8')],
    )

    return response


if __name__ == "__main__":
    print("Starting Worker C (format capability)...")
    app.run(host="0.0.0.0", port=50053)
